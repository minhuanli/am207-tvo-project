{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from scipy.stats import norm, gaussian_kde\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mi(vae, x_validation, device='cpu', S=1):\n",
    "    '''Approximate the mutual information between x and z\n",
    "    I(x, z) = E_xE_{q(z|x)}log(q(z|x)) - E_xE_{q(z|x)}log(q(z))\n",
    "    Modified from the implementation by Author of the paper \"LAGGING INFERENCE NETWORKS \n",
    "    AND POSTERIOR COLLAPSE IN VARIATIONAL AUTOENCODERS\"\n",
    "    see https://github.com/jxhe/vae-lagging-encoder/blob/master/modules/encoders/encoder.py\n",
    "    \n",
    "    This function will calculate the mutual information during the training with LIN trick,\n",
    "    as a criterion wehther we should stop the agressive training. \n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    vae: A vae instance, with .infer() method\n",
    "    \n",
    "    x_validation: Validation X data set\n",
    "    \n",
    "    Returns: Float\n",
    "    '''\n",
    "    N_batch = x_validation.shape[0]\n",
    "    \n",
    "    #infer zs with encoder \n",
    "    # 2D Tensor, shape [N_batch, z_dim]\n",
    "    mean, std = self.infer(x_validation)\n",
    "    assert std.shape == (N_batch, vae.z_dim)\n",
    "    assert mean.shape == (N_batch, vae.z_dim)\n",
    "    \n",
    "    ## Term 1: calculate Negative Entropy, E_{q(z|x)}log(q(z|x))\n",
    "    # E_{q(z|x)}log(q(z|x)) = -0.5* z_dim *log(2*\\pi) - 0.5*(1+log(std**2)).sum(-1)\n",
    "    # 1D Tensor, shape [N_batch]\n",
    "    neg_entropy = (-0.5 * vae.z_dim * math.log(2. * math.pi)- 0.5 * (1 + torch.log(std**2)).sum(-1))\n",
    "    \n",
    "    \n",
    "    ## Term 2: calculate E_{q(z|x)}log(q(z))\n",
    "    #sample zs with the parameters\n",
    "    if device == 'cuda': z_samples = torch.normal(0,1,size=(S, N_batch, vae.z_dim)).cuda() * std + mean\n",
    "    if device == 'cpu': z_samples = torch.normal(0,1,size=(S, N_batch, vae.z_dim)) * std + mean\n",
    "    assert z_samples.shape == (S, N_batch, vae.z_dim)\n",
    "    \n",
    "    #evaluate sampled z's under variational distribution\n",
    "    # 2D Tensor, shape [S, N_batch]\n",
    "    norm1 = torch.distributions.Normal(mean, std)\n",
    "    log_qz= torch.sum(norm1.log_prob(z_samples), axis=-1)\n",
    "    \n",
    "    return (torch.mean(neg_entropy) - torch.mean(log_qz)).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
